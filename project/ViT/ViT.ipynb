{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda, Resize\n",
    "from medmnist import INFO, TissueMNIST, PneumoniaMNIST, DermaMNIST, OrganAMNIST\n",
    "from transformers import ViTForImageClassification\n",
    "from transformers import AdamW\n",
    "\n",
    "def load_data(data_flag='pneumoniamnist', batch_size=16):\n",
    "    n_classes = len(INFO[data_flag]['label'])\n",
    "    transform = Compose([\n",
    "        Resize((224, 224)),\n",
    "        ToTensor(),\n",
    "        Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "        Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "    ])\n",
    "\n",
    "    train_dataset = PneumoniaMNIST(split='train', transform=transform, download=True)\n",
    "    val_dataset = PneumoniaMNIST(split='val', transform=transform, download=True)\n",
    "    test_dataset = PneumoniaMNIST(split='test', transform=transform, download=True)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, n_classes\n",
    "\n",
    "def setup_model(n_classes):\n",
    "    model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=n_classes)\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, device):\n",
    "    print(f'Using device: {device}')\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_model_path = 'organA224.pth'\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(10):\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.squeeze(1).long()\n",
    "\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        total, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                labels = labels.squeeze(1).long()\n",
    "\n",
    "                outputs = model(images).logits\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Accuracy: {accuracy}%')\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Best model saved with accuracy: {accuracy}% at epoch {epoch+1}\")\n",
    "\n",
    "    print(f'Training complete. Best model was saved with an accuracy of {best_accuracy}%.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    #device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    train_loader, val_loader, test_loader, n_classes = load_data()\n",
    "    model = setup_model(n_classes)\n",
    "    train_and_evaluate(model, train_loader, val_loader, device)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #main()\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/oscarrosman/.medmnist/pneumoniamnist.npz\n",
      "Using downloaded and verified file: /Users/oscarrosman/.medmnist/pneumoniamnist.npz\n",
      "Using downloaded and verified file: /Users/oscarrosman/.medmnist/pneumoniamnist.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.64%\n",
      "Validation Accuracy: 97.71%\n",
      "Test Accuracy: 87.66%\n"
     ]
    }
   ],
   "source": [
    "def EvaluateModel(loader, model, device, label):\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.squeeze(1).long()\n",
    "        outputs = model(images).logits\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        #print('Predicted: ', predicted)\n",
    "        #print('Actual: ', labels)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'{label} Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "train_loader, val_loader, test_loader, n_classes = load_data()\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=n_classes)\n",
    "model.load_state_dict(torch.load('pneumonia224.pth'))\n",
    "EvaluateModel(train_loader, model, device, 'Training')\n",
    "EvaluateModel(val_loader, model, device, 'Validation')\n",
    "EvaluateModel(test_loader, model, device, 'Test')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Assuming `image` is a single image tensor of shape (batch_size, channels, height, width)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(val_loader))[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Taking one image from validation loader\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mvisualize_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, label \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m     34\u001b[0m     image, label \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device), label\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m, in \u001b[0;36mvisualize_attention\u001b[0;34m(model, image)\u001b[0m\n\u001b[1;32m     12\u001b[0m     attention_weights \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mattentions  \u001b[38;5;66;03m# Extract attention weights\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Plot attention maps\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mattention_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m num_heads \u001b[38;5;241m=\u001b[39m attention_weights[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(num_layers, num_heads, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m15\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_attention(model, image):\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Forward pass to get attention weights\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        attention_weights = outputs.attentions  # Extract attention weights\n",
    "    \n",
    "    # Plot attention maps\n",
    "    num_layers = len(attention_weights)\n",
    "    num_heads = attention_weights[0].size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(num_layers, num_heads, figsize=(15, 15))\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        for head in range(num_heads):\n",
    "            sns.heatmap(attention_weights[layer][0, head].cpu().numpy(), ax=axs[layer, head], cmap=\"viridis\", cbar=False)\n",
    "            axs[layer, head].set_title(f\"Layer {layer+1}, Head {head+1}\")\n",
    "            axs[layer, head].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming `image` is a single image tensor of shape (batch_size, channels, height, width)\n",
    "image = next(iter(val_loader))[0][0].unsqueeze(0).to(device)  # Taking one image from validation loader\n",
    "visualize_attention(model, image)\n",
    "\n",
    "for image, label in val_loader:\n",
    "    image, label = image.to(device), label.to(device)\n",
    "    visualize_attention(model, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
